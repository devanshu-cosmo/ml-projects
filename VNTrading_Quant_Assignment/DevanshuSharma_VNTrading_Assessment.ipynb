{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5d5e9ca-7cfa-4c32-ac64-20f989b1d2be",
   "metadata": {},
   "source": [
    "# Quantitative Researcher Assignment - VNTrading\n",
    "## Financial Time Series Analysis - Devanshu Sharma\n",
    "- Problem Statement: Create a SMA crossover trading strategy for BTC data. Test the strategy using backtesting\n",
    "- More details provided in the readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63daaa4d-6455-46d4-b781-86ee1ebe5c18",
   "metadata": {},
   "source": [
    "### Import standard libraries & Set up plotting styles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc57e3d7-d27b-4e49-9b90-25975211753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series analysis\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import arch\n",
    "import pandas_ta_classic as ta\n",
    "\n",
    "# Machine learning and statistical models\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Financial specific\n",
    "import yfinance as yf  # fallback for additional data if needed\n",
    "\n",
    "# Performance optimization\n",
    "from numba import jit\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0444de16-c473-4873-9086-15d7cec61fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53a29b-525d-4be0-95f4-adf9c72fc639",
   "metadata": {},
   "source": [
    "### Data Extraction & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35880a63-1ceb-448b-9685-2ae15073ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path=None, parquet_path=None):\n",
    "    try:\n",
    "        if parquet_path:\n",
    "            df = pd.read_parquet(parquet_path)\n",
    "            print(\"Loaded Parquet file\")\n",
    "        elif csv_path:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(\"Loaded CSV file\")\n",
    "        else:\n",
    "            raise ValueError(\"No file path provided\")\n",
    "            \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# def preprocess_time_series(df, date_column=None, value_columns=None):\n",
    "#     df_clean = df.copy()\n",
    "    \n",
    "#     # Set datetime index\n",
    "#     if date_column:\n",
    "#         df_clean[date_column] = pd.to_datetime(df_clean[date_column])\n",
    "#         df_clean = df_clean.set_index(date_column)\n",
    "#         df_clean = df_clean.sort_index()\n",
    "#         print(f\"Set datetime index: {date_column}\")\n",
    "    \n",
    "#     # Handle missing values\n",
    "#     initial_missing = df_clean.isnull().sum().sum()\n",
    "#     if initial_missing > 0:\n",
    "#         # For time series, use forward fill then backward fill\n",
    "#         df_clean = df_clean.ffill().bfill()\n",
    "#         remaining_missing = df_clean.isnull().sum().sum()\n",
    "#         # print(f\"Missing values removed: {initial_missing}\")\n",
    "    \n",
    "#     # Remove constant columns\n",
    "#     constant_columns = [col for col in df_clean.columns if df_clean[col].nunique() <= 1]\n",
    "#     if constant_columns:\n",
    "#         df_clean = df_clean.drop(columns=constant_columns)\n",
    "#         # print(f\"Removed constant columns: {constant_columns}\")\n",
    "    \n",
    "#     return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b2fba-dbfb-433e-b535-db0ceac249da",
   "metadata": {},
   "source": [
    "### Print Data Summary & Statistics\n",
    "- Preprocessing not needed as the data is clean and no null values\n",
    "- Timestamp format in Unix with milliseconds units; convert properly into more intuitive format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812e2072-2b84-4181-af4c-7f90e519c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_price</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-24 00:00:00</th>\n",
       "      <td>39881.95</td>\n",
       "      <td>439.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24 00:01:00</th>\n",
       "      <td>39926.55</td>\n",
       "      <td>776.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24 00:02:00</th>\n",
       "      <td>39877.05</td>\n",
       "      <td>339.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24 00:03:00</th>\n",
       "      <td>39927.45</td>\n",
       "      <td>246.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24 00:04:00</th>\n",
       "      <td>39926.55</td>\n",
       "      <td>423.920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mid_price   volume\n",
       "datetime                               \n",
       "2024-01-24 00:00:00   39881.95  439.561\n",
       "2024-01-24 00:01:00   39926.55  776.492\n",
       "2024-01-24 00:02:00   39877.05  339.306\n",
       "2024-01-24 00:03:00   39927.45  246.511\n",
       "2024-01-24 00:04:00   39926.55  423.920"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1439, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1439 entries, 2024-01-24 00:00:00 to 2024-01-24 23:58:00\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   mid_price  1439 non-null   float64\n",
      " 1   volume     1439 non-null   float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 33.7 KB\n",
      "None\n",
      "          mid_price       volume\n",
      "count   1439.000000  1439.000000\n",
      "mean   39919.132279   189.511495\n",
      "std      175.222416   223.523507\n",
      "min    39516.950000    13.454000\n",
      "25%    39777.600000    73.456500\n",
      "50%    39917.850000   123.278000\n",
      "75%    40047.700000   213.539500\n",
      "max    40479.550000  2464.729000\n",
      "\n",
      "Missing values per column:\n",
      "mid_price    0\n",
      "volume       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"BTCUSDT_price_data_2024-01-24.csv\")\n",
    "# df = preprocess_time_series(df, date_column=\"timestamp\")\n",
    "\n",
    "####\n",
    "# Remove the trailing 'T', convert to integer, then to datetime\n",
    "df['timestamp'] = df['timestamp'].str.replace('T', '').astype(np.int64)\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "# Remove the 'timestamp' column from the dataframe\n",
    "df.drop(columns=['timestamp'], inplace=True)\n",
    "###\n",
    "# set as index for time-series convenience\n",
    "df.set_index('datetime', inplace=True)\n",
    "####\n",
    "\n",
    "def data_overview(df):\n",
    "    display(df.head())\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(df.info())\n",
    "    print(df.describe())\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "data_overview(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da92ef-6a44-4aa5-8689-bd59f100130a",
   "metadata": {},
   "source": [
    "### Visualize the preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4302d8-a099-44b3-830d-65f4ec30af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_analysis(df, target_columns=None):\n",
    "\n",
    "    if target_columns is None:\n",
    "        target_columns = df.select_dtypes(include=[np.number]).columns[:3]  # First 3 numeric columns\n",
    "    \n",
    "    n_plots = len(target_columns)\n",
    "    fig, axes = plt.subplots(n_plots, 3, figsize=(20, 5*n_plots))\n",
    "    \n",
    "    if n_plots == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(target_columns):\n",
    "        # Time series plot\n",
    "        axes[i, 0].plot(df.index, df[col])\n",
    "        axes[i, 0].set_title(f'{col} - Time Series')\n",
    "        axes[i, 0].set_ylabel('Value')\n",
    "        axes[i, 0].grid(True)\n",
    "        \n",
    "        # Distribution\n",
    "        axes[i, 1].hist(df[col].dropna(), bins=50, alpha=0.7, density=True)\n",
    "        axes[i, 1].set_title(f'{col} - Distribution')\n",
    "        axes[i, 1].set_ylabel('Density')\n",
    "        \n",
    "        # ACF plot\n",
    "        plot_acf(df[col].dropna(), ax=axes[i, 2], lags=40)\n",
    "        axes[i, 2].set_title(f'{col} - Autocorrelation')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(\"preprocessed_data_visuals.pdf\", format=\"pdf\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def correlation_analysis(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    corr_matrix = numeric_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "plot_time_series_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a12b5a-45fb-4c09-8be5-9f7f135e01af",
   "metadata": {},
   "source": [
    "### Feature Engineering & Feature Selection\n",
    "- Create Features \n",
    "- Select the most relevant features to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13be67e6-1eaa-4373-96e6-fafdb327b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment for more robust analysis but first perform the minimal asked analysis\n",
    "# def create_features(df, price_column=None):\n",
    "#     df_features = df.copy()\n",
    "    \n",
    "#     numeric_columns = df_features.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "#     for col in numeric_columns:\n",
    "#         # Returns\n",
    "#         df_features[f'{col}_returns'] = df_features[col].pct_change()\n",
    "#         # df_features[f'{col}_log_returns'] = np.log(df_features[col] / df_features[col].shift(1))\n",
    "        \n",
    "#         # Rolling statistics\n",
    "#         for window in [5, 10, 20]:  # Common financial windows\n",
    "#             df_features[f'{col}_rolling_mean_{window}'] = df_features[col].rolling(window=window).mean()\n",
    "#             df_features[f'{col}_rolling_std_{window}'] = df_features[col].rolling(window=window).std()\n",
    "#             df_features[f'{col}_rolling_min_{window}'] = df_features[col].rolling(window=window).min()\n",
    "#             df_features[f'{col}_rolling_max_{window}'] = df_features[col].rolling(window=window).max()\n",
    "            \n",
    "#             # Volatility (annualized if daily data)\n",
    "#             df_features[f'{col}_volatility_{window}'] = df_features[col].rolling(window=window).std() * np.sqrt(252)\n",
    "        \n",
    "#         # Technical indicators\n",
    "#         df_features[f'{col}_momentum'] = df_features[col] / df_features[col].shift(5) - 1\n",
    "#         df_features[f'{col}_MA_ratio'] = df_features[col] / df_features[f'{col}_rolling_mean_20']\n",
    "        \n",
    "#         # Statistical features\n",
    "#         df_features[f'{col}_zscore'] = (df_features[col] - df_features[col].mean()) / df_features[col].std()\n",
    "    \n",
    "#     # Handle infinite values and NaNs created by transformations\n",
    "#     df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
    "#     df_features = df_features.ffill().bfill()\n",
    "    \n",
    "#     print(f\" Created {len(df_features.columns) - len(df.columns)} additional features\")\n",
    "#     return df_features\n",
    "\n",
    "def create_features(df, sma_period = 10):\n",
    "    df_indicators = df.copy()\n",
    "    # Calculate 1-minute returns\n",
    "    df_indicators['returns'] = df_indicators['mid_price'].pct_change()\n",
    "    \n",
    "    # Calculate 10-period Simple Moving Average (SMA)\n",
    "    df_indicators['sma_10'] = df_indicators['mid_price'].rolling(window=sma_period).mean()\n",
    "    \n",
    "    # Calculate additional useful metrics for analysis\n",
    "    df_indicators['price_sma_diff'] = df_indicators['mid_price'] - df_indicators['sma_10']\n",
    "    df_indicators['price_sma_ratio'] = df_indicators['mid_price'] / df_indicators['sma_10']\n",
    "\n",
    "    \n",
    "    return df_indicators\n",
    "\n",
    "# select only the relevant features\n",
    "def select_features(df, ticker):\n",
    "    features = df[ticker].dropna()\n",
    "    return features\n",
    "\n",
    "all_features = create_features(df)\n",
    "\n",
    "# Add more tickers if needed\n",
    "selected_tickers = [\"returns\", \"sma_10\"]\n",
    "selected_features = select_features(all_features, selected_tickers)\n",
    "df = pd.concat([df,selected_features], ignore_index=True)\n",
    "\n",
    "# Handle infinite values and NaNs created by transformations\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80350c85-8c87-42f6-bebf-cb8da9931cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>returns</th>\n",
       "      <th>sma_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39881.95</td>\n",
       "      <td>439.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39926.55</td>\n",
       "      <td>776.492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39877.05</td>\n",
       "      <td>339.306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39927.45</td>\n",
       "      <td>246.511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39926.55</td>\n",
       "      <td>423.920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>40140.405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>40137.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>40131.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>40124.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>40116.960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2869 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mid_price   volume   returns     sma_10\n",
       "0      39881.95  439.561       NaN        NaN\n",
       "1      39926.55  776.492       NaN        NaN\n",
       "2      39877.05  339.306       NaN        NaN\n",
       "3      39927.45  246.511       NaN        NaN\n",
       "4      39926.55  423.920       NaN        NaN\n",
       "...         ...      ...       ...        ...\n",
       "2864        NaN      NaN  0.000601  40140.405\n",
       "2865        NaN      NaN -0.000947  40137.620\n",
       "2866        NaN      NaN -0.000244  40131.630\n",
       "2867        NaN      NaN -0.000619  40124.000\n",
       "2868        NaN      NaN  0.000399  40116.960\n",
       "\n",
       "[2869 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c93f99-4a40-44f4-a1fc-fedcef37a6b1",
   "metadata": {},
   "source": [
    "### Generate signals for buy-sell calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7933bdd-761f-472d-85b8-5861e29fd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_signals(df_indicators):\n",
    "    df_signals = df_indicators.copy()\n",
    "    \n",
    "    # Initialize signal columns\n",
    "    df_signals['signal'] = 0  # 0 = no signal, 1 = buy, -1 = sell\n",
    "    df_signals['position'] = 0  # 1 = long, -1 = short, 0 = flat\n",
    "    df_signals['crossover'] = 0  # Track crossover events\n",
    "    \n",
    "    # Method 1: Detect crossovers using price-SMA difference sign changes\n",
    "    # This is more robust than simple comparison at each point\n",
    "    \n",
    "    # Calculate the difference and its sign\n",
    "    df_signals['price_sma_diff'] = df_signals['mid_price'] - df_signals['sma_10']\n",
    "    df_signals['prev_price_sma_diff'] = df_signals['price_sma_diff'].shift(1)\n",
    "    \n",
    "    # Detect crossovers: sign change from negative to positive (price crosses above SMA)\n",
    "    buy_condition = (df_signals['prev_price_sma_diff'] < 0) & (df_signals['price_sma_diff'] > 0)\n",
    "    \n",
    "    # Detect crossunders: sign change from positive to negative (price crosses below SMA)  \n",
    "    sell_condition = (df_signals['prev_price_sma_diff'] > 0) & (df_signals['price_sma_diff'] < 0)\n",
    "    \n",
    "    # Apply signals\n",
    "    df_signals.loc[buy_condition, 'signal'] = 1    # Buy signal\n",
    "    df_signals.loc[sell_condition, 'signal'] = -1  # Sell signal\n",
    "    df_signals.loc[buy_condition | sell_condition, 'crossover'] = 1\n",
    "    \n",
    "    return df_signals\n",
    "\n",
    "df_signals = generate_trading_signals(select_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11ed03-6ef6-489e-a1e6-6a6cdf476626",
   "metadata": {},
   "source": [
    "### Setup the forecasting model\n",
    "- Multiple choices provided\n",
    "- Select the best one according to the time constraints and underlying data\n",
    "- Comment out the useless models to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fb465-b97e-4aac-9fdc-54f32fbcec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_models(self, X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        # 'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(random_state=42)\n",
    "        # 'LightGBM': lgb.LGBMRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "        test_r2 = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'test_r2': test_r2,\n",
    "            'predictions': y_pred_test\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - Test RMSE: {test_rmse:.6f}, R²: {test_r2:.6f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bfd27-d408-41a4-a36b-ddaa815d5e39",
   "metadata": {},
   "source": [
    "### Saving the results externally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cebae-bb43-457e-8846-475856c6ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_dict, filename_prefix='DevanshuSharma_VNTrading_results'):\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    for name, result in results_dict.items():\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            result.to_csv(f'{filename_prefix}_{name}_{timestamp}.csv', index=False)\n",
    "        elif isinstance(result, plt.Figure):\n",
    "            result.savefig(f'{filename_prefix}_{name}_{timestamp}.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(f\"Saved externally as: {filename_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de4290-b586-4703-989c-058b6b77fe73",
   "metadata": {},
   "source": [
    "## Less Relevant Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7a638-c255-4a55-9517-e6766dbf4c5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (Optional) Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c513ba2-5fb9-4213-8e93-27f984a3df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactive_time_series(df, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for col in columns:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df.index,\n",
    "            y=df[col],\n",
    "            name=col,\n",
    "            mode='lines'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Time Series Analysis\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Value\",\n",
    "        hovermode='x unified',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "plot_interactive_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bda7c-a785-430d-963e-ffc1f9bab2cd",
   "metadata": {},
   "source": [
    "### Check stationarity of the time series data\n",
    "- A time series data needs to be stationary in order for ARIMA model to work\n",
    "- There are no such restrictions for SARIMA / SARIMAX models or Machine Learning Models\n",
    "- However, it's a good practice to check for the stationarity of a time series data\n",
    "- Determined with ADF test: if the p-value < 0.05, you reject the null hypothesis (that the series is non-stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ed6d8-698f-4ce5-9e33-992c85e160b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_analysis(df, columns=None):\n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for col in columns:\n",
    "        series = df[col].dropna()\n",
    "        \n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(series)\n",
    "        adf_stat, adf_pvalue = adf_result[0], adf_result[1]\n",
    "        \n",
    "        # KPSS test\n",
    "        try:\n",
    "            kpss_result = kpss(series, regression='c')\n",
    "            kpss_stat, kpss_pvalue = kpss_result[0], kpss_result[1]\n",
    "        except:\n",
    "            kpss_stat, kpss_pvalue = np.nan, np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'Series': col,\n",
    "            'ADF_Statistic': adf_stat,\n",
    "            'ADF_pvalue': adf_pvalue,\n",
    "            'ADF_Stationary': adf_pvalue < 0.05\n",
    "            # 'KPSS_Statistic': kpss_stat,\n",
    "            # 'KPSS_pvalue': kpss_pvalue,\n",
    "            # 'KPSS_Stationary': kpss_pvalue > 0.05 if not np.isnan(kpss_pvalue) else np.nan\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "stationarity_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1a186-64d0-4c4c-9a18-0002d2c64e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
