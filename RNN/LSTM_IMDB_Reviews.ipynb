{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f00f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, root_mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, accuracy_score, classification_report, silhouette_score, precision_recall_curve\n",
    "from sklearn.model_selection import learning_curve, train_test_split\n",
    "from scipy.stats import normaltest\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas_ta as ta\n",
    "import xgboost as xgb\n",
    "from fredapi import Fred\n",
    "\n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first download data from imdb\n",
    "review_size = 20000 #set an upper limit on the length of review as a feature\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=review_size) \n",
    "\n",
    "#the data is preprocessed, each integer represents a word\n",
    "#pad the reviews to fix the data length for the neural network\n",
    "padding_length = 2000\n",
    "X_train, X_test = pad_sequences(X_train, maxlen=padding_length, padding=\"post\", truncating=\"post\"), pad_sequences(X_test, maxlen=padding_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "#perform embedding and also build the LSTM model\n",
    "def build_LSTM():\n",
    "    LSTM_model = keras.Sequential([Input(shape=(padding_length,)),\n",
    "        Embedding(input_dim=review_size, output_dim=128, input_length=padding_length),\n",
    "        \n",
    "       # Bidirectional wrapper processes sequence both directions\n",
    "    Bidirectional(LSTM(units=128, \n",
    "                       return_sequences=False,\n",
    "                       dropout=0.25,         # Regularization\n",
    "                       recurrent_dropout=0.25)),\n",
    "    \n",
    "    # Fully connected layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.25),                               \n",
    "                                   \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "      ])\n",
    "    \n",
    "    LSTM_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "                      loss=\"binary_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "    \n",
    "    return LSTM_model\n",
    "\n",
    "LSTM_model = build_LSTM()\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e3b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = EarlyStopping(monitor=\"val_loss\", patience=1) #patience sets the number of epochs of no change in val_loss to wait\n",
    "\n",
    "#train the LSTM model\n",
    "\n",
    "training_LSTM = LSTM_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2, verbose=1, callbacks=[stop_early])#, callbacks=[stop_early]\n",
    "\n",
    "#visualize the results of training\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(training_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune the threshold for separating positive and negative reviews\n",
    "#check and compute the class imbalance\n",
    "#higher precision pushes the threshold to higher values\n",
    "#higher recall pulls the threshold to lower values\n",
    "\n",
    "y_train_probability = LSTM_model.predict(X_train).ravel()\n",
    "plt.hist(y_train_probability,log=True, bins=30)\n",
    "\n",
    "def set_threshold(switch):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train, y_train_probability)\n",
    "\n",
    "    #maximize the F1 score to achieve precision--recall balance\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    optimal_threshold_f1 = thresholds[np.argmax(f1_scores)]\n",
    "    print(f\"Optimal threshold for maximized F1 score: {optimal_threshold_f1:.3f}\")\n",
    "\n",
    "    #Other approach to compute optimal threshold\n",
    "    #try to aim for as high precision as possible, avoid false positives\n",
    "    precision_goal = 0.75\n",
    "    optimal_threshold_precision = thresholds[np.where(precision >= precision_goal)[0][0]]\n",
    "    print(f\"Optimal threshold for higher precision: {optimal_threshold_precision:.3f}\")\n",
    "    \n",
    "    if switch==\"f1\":\n",
    "        return optimal_threshold_f1\n",
    "    elif switch==\"precision\":\n",
    "        return optimal_threshold_precision\n",
    "    else:\n",
    "        return 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test dataset results\n",
    "score_LSTM = LSTM_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", score_LSTM[0])\n",
    "print(\"Test Accuracy:\", score_LSTM[1])\n",
    "\n",
    "#predict the category using the below function\n",
    "\n",
    "#first create the mapping from word to digits\n",
    "char_digit = imdb.get_word_index()\n",
    "digit_char_index = {a:b for b,a in char_digit.items()} \n",
    "\n",
    "def review_decrypt(encrypted_review): #takes in the encrypted vectorized review and decodes it to human readable format\n",
    "    return ' '.join([digit_char_index.get(i-3, '?') for i in encrypted_review])\n",
    "\n",
    "def review_predict(text):\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    # Step 1: Tokenize text (split into words and convert to lowercase)\n",
    "    tokens = tf.keras.preprocessing.text.text_to_word_sequence(text, lower=True)\n",
    "    \n",
    "    \n",
    "    # Step 2: Convert words to integers using IMDB's word index\n",
    "    tokens = [char_digit.get(word, 0) for word in tokens]  # 0 = unknown word\n",
    "    tokens = [min(idx, review_size - 1) for idx in tokens]  # Force indices to be < vocab_size\n",
    "#     print(type(tokens))\n",
    "    # Step 3: Pad/truncate to match model's input length (500)\n",
    "    padded = pad_sequences([tokens], maxlen=padding_length, padding='post', truncating='post')\n",
    "#     print(type(padded))\n",
    "    # Step 4: Predict sentiment (output is between 0 and 1)\n",
    "    pred = LSTM_model.predict(padded, verbose=0)[0][0]\n",
    "        # Step 5: Interpret prediction\n",
    "    sentiment = \"Positive\" if pred >= optimal_threshold else \"Negative\"\n",
    "    confidence = pred #if pred > 0.5 else 1 - pred\n",
    "    \n",
    "    return f\"{sentiment} (Confidence: {confidence:.2f})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1956ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# custom_reviews = [\n",
    "#     \"good movie\",\n",
    "#     \"worst worst worst movie\",\n",
    "#     \"this movie was fantastic! the acting was brilliant.\",\n",
    "#     \"terrible plot and awful acting waste of time.\",\n",
    "#     \"it was okay, but the ending could have been better.\",\n",
    "#     \"the plot was boring and the acting was terrible\",\n",
    "#     \"do not waste your money on this movie\"\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "custom_reviews = [\n",
    "    \"Christopher Nolan’s The Dark Knight redefines the superhero genre with its gripping narrative, intense action, and Heath Ledger’s iconic performance as the Joker. The film balances psychological depth with blockbuster spectacle, exploring themes of chaos and morality. Hans Zimmer’s score elevates every scene, and the pacing never falters. Minor flaws? Some may find the runtime long, but it’s a small price for such a rich story. A timeless classic.\",\n",
    "    \"James Cameron’s Avatar delivers groundbreaking visuals but suffers from a clichéd plot and shallow characters. The world-building is impressive, yet the story feels like a rehash of Pocahontas in space. Dialogue is often cringe-worthy. Worth watching once for the spectacle, but it doesn’t hold up on repeat viewings.\",\n",
    "    \"Pixar’s Toy Story 3 tugs at heartstrings with its emotional depth and clever humor. The animation is flawless, and the themes of growing up resonate universally. The climax in the incinerator is one of cinema’s most tense moments. Only gripe? Some jokes feel recycled from earlier films. Still, a near-perfect trilogy finale.\",\n",
    "    \"Tommy Wiseau’s The Room is a trainwreck of incoherent plotting, wooden acting, and baffling production choices. The dialogue feels AI-generated, and the green-screen backdrops are laughable. Yet, its sheer incompetence has spawned a cult following. Watch with friends for ironic laughs—alone, it’s unbearable.\",\n",
    "    \"Nolan’s Inception dazzles with its dream-within-a-dream concept and stunning visuals (e.g., the rotating hallway fight). However, the emotional core feels underdeveloped—Cobb and Mal’s relationship lacks depth. The supporting cast (Hardy, Page) shines, but some characters are thinly written. A technical marvel that prioritizes intellect over heart.\"\n",
    "]\n",
    "\n",
    "for review in custom_reviews:\n",
    "#     print(f\"Review: {review[:60]}...\")\n",
    "    print(f\"Prediction: {review_predict(review)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921de100",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
